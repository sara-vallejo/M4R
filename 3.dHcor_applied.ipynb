{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import pairwise_distances, pairwise_kernels\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning) \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv(r'/Users/saravallejomengod/MathsYear4/M4R/utils/wb_info.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "goals = list(info[3].unique())\n",
    "goals.remove('T')\n",
    "dict_goals = {}\n",
    "\n",
    "for goal in goals:\n",
    "    g = info[4].where(info[3] == goal)\n",
    "\n",
    "    dict_goals[goal] = [t for t in g if str(t) != 'nan']\n",
    "    dict_goals[goal] = list(set(dict_goals[goal]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to compute dHSIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def width(Z):\n",
    "    \"\"\"\n",
    "    Computes the median heuristic for the kernel bandwidth\n",
    "    \"\"\"\n",
    "    dist_mat = pairwise_distances(Z, metric='euclidean')\n",
    "    width_Z = np.median(dist_mat[dist_mat > 0])\n",
    "    return width_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_K_list(X_list, n_samples, n_nodes):\n",
    "    \"\"\"\n",
    "    Computes the kernel matrices of the variables in X_array, where each column represents one variable.\n",
    "    Returns a list of the kernel matrices of each variable.\n",
    "    \"\"\"\n",
    "    k_list = list(pairwise_kernels(X_list[i], metric='rbf', gamma=0.5/(width(X_list[i])**2)) for i in range(n_nodes))\n",
    "    return k_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations(iterable, r):\n",
    "    # combinations('ABCD', 2) --> AB AC AD BC BD CD\n",
    "    # combinations(range(4), 3) --> 012 013 023 123\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    if r > n:\n",
    "        return\n",
    "    indices = list(range(r))\n",
    "    yield list(pool[i] for i in indices)\n",
    "    while True:\n",
    "        for i in reversed(range(r)):\n",
    "            if indices[i] != i + n - r:\n",
    "                break\n",
    "        else:\n",
    "            return\n",
    "        indices[i] += 1\n",
    "        for j in range(i+1, r):\n",
    "            indices[j] = indices[j-1] + 1\n",
    "        yield list(pool[i] for i in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations_tuple(iterable, r):\n",
    "    # combinations('ABCD', 2) --> AB AC AD BC BD CD\n",
    "    # combinations(range(4), 3) --> 012 013 023 123\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    if r > n:\n",
    "        return\n",
    "    indices = list(range(r))\n",
    "    yield tuple(pool[i] for i in indices)\n",
    "    while True:\n",
    "        for i in reversed(range(r)):\n",
    "            if indices[i] != i + n - r:\n",
    "                break\n",
    "        else:\n",
    "            return\n",
    "        indices[i] += 1\n",
    "        for j in range(i+1, r):\n",
    "            indices[j] = indices[j-1] + 1\n",
    "        yield tuple(pool[i] for i in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dHSIC(k_list):\n",
    "    \"\"\"\n",
    "    Computes the dHSIC statistic\n",
    "    \"\"\"\n",
    "    n_nodes = len(k_list)\n",
    "    n_samples = k_list[0].shape[0]\n",
    "    \n",
    "    term1, term2, term3 = 1, 1, 2/n_samples    \n",
    "    for j in range(n_nodes):\n",
    "        term1 = term1 * k_list[j]\n",
    "        term2 = term2 * np.sum(k_list[j]) / (n_samples**2)\n",
    "        term3 = term3 * np.sum(k_list[j], axis=0) / n_samples\n",
    "    term1_sum = np.sum(term1)\n",
    "    term3_sum = np.sum(term3)\n",
    "    dHSIC = term1_sum/(n_samples**2) + term2 - term3_sum\n",
    "    return dHSIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dHSIC_permutation_MC(k_list, n_samples, n_nodes, stat_found, n_perms=5000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Approximates the null distribution by permutating all variables. Using Monte Carlo approximation.\n",
    "    \"\"\"\n",
    "    # initiating statistics\n",
    "    statistics = np.zeros(n_perms)\n",
    "    \n",
    "    for i in range(n_perms):\n",
    "        term1 = k_list[0]\n",
    "        term2 = np.sum(k_list[0])/(n_samples**2)\n",
    "        term3 = 2 * np.sum(k_list[0], axis=0) / (n_samples**2)\n",
    "\n",
    "        for j in range(1, n_nodes):\n",
    "            index_perm = np.random.permutation(k_list[j].shape[0])\n",
    "            k_perm = k_list[j][index_perm, index_perm[:, None]]\n",
    "\n",
    "            term1 = term1 * k_perm\n",
    "            term2 = term2 * np.sum(k_perm) / (n_samples**2)\n",
    "            term3 = term3 * np.sum(k_perm, axis=0) / n_samples\n",
    "\n",
    "        term1_sum = np.sum(term1)\n",
    "        term3_sum = np.sum(term3)\n",
    "\n",
    "        statistics[i] = term1_sum/(n_samples**2) + term2 - term3_sum\n",
    "    \n",
    "    statistics_sort = np.sort(statistics)\n",
    "    # computing 1-alpha critical value\n",
    "    Bind = np.sum(stat_found==statistics_sort) + int(np.ceil((1-alpha)*(n_perms+1)))\n",
    "    critical_value = statistics_sort[Bind]\n",
    "    \n",
    "    return critical_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_independence_test_MC(k_list, n_perms=5000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs the independence test with HSIC and returns an accept or reject statement\n",
    "    \n",
    "    Inputs:\n",
    "    k_list: list of Kernel matrices for each variable, each having dimensions (n_samples, n_samples)  \n",
    "    n_perms: number of permutations performed when bootstrapping the null\n",
    "    alpha: rejection threshold of the test\n",
    "    \n",
    "    Returns:\n",
    "    reject: 1 if null rejected, 0 if null not rejected\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_nodes = len(k_list)\n",
    "    n_samples = k_list[0].shape[0]\n",
    "    \n",
    "    \n",
    "    # statistic and threshold\n",
    "    stat = dHSIC(k_list)\n",
    "    critical_value = dHSIC_permutation_MC(k_list, n_samples, n_nodes, stat) \n",
    "    \n",
    "    reject = int(stat > critical_value)\n",
    "    \n",
    "    return stat, reject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HSIC_normalised_pairwise(group, groups_data, iterable):\n",
    "    \"\"\"\n",
    "    Perform the independence test with HSIC for all possible pairwise combinations. If null hypothesis\n",
    "    is rejected, compute the normalised HSIC statistic and set this as the weight of the edge between\n",
    "    both variables (nodes of the graph).\n",
    "    \"\"\"\n",
    "    #For given dictionary groups_data, take nd.array corresponding to group\n",
    "    # ie. continents_prep_g_K['Europe']\n",
    "    group_arr = groups_data[group]\n",
    "    \n",
    "    K = len(iterable)   #number of total variables (17 goals, 76 targets)\n",
    "    edges = {}          #initialize dictionary with edges according to normalised HSIC\n",
    "    Adj = np.eye(K)  #initialize KxK adjacency matrix \n",
    "    \n",
    "    indexes = np.arange(K)    #create vector corresponding to indexes of iterable\n",
    "    #find all possible 2-combinations of indexes without order\n",
    "    g_combinations = list(combinations_tuple(indexes, 2))   \n",
    "    \n",
    "    #compute individual HSIC_XX\n",
    "    HSIC_XX_sqrt = []\n",
    "    for i in indexes:\n",
    "        k_list_XX = list((group_arr[i], group_arr[i]))\n",
    "        HSIC_XX = dHSIC(k_list_XX)\n",
    "        HSIC_XX_sqrt.append(np.sqrt(HSIC_XX))\n",
    "    \n",
    "    for comb in g_combinations: \n",
    "        #create k_list[i] = Kernel from observed data for variable comb[i]\n",
    "        k_list = list((group_arr[comb[0]], group_arr[comb[1]]))\n",
    "        \n",
    "        HSIC_XY = dHSIC(k_list)\n",
    "        critical_value = dHSIC_permutation_MC(k_list, k_list[0].shape[0], 2, HSIC_XY)\n",
    "        \n",
    "        if HSIC_XY > critical_value:\n",
    "            HSIC_norm = HSIC_XY/(HSIC_XX_sqrt[comb[0]]*HSIC_XX_sqrt[comb[1]])\n",
    "            Adj[comb[0], comb[1]] = HSIC_norm\n",
    "            Adj[comb[1], comb[0]] = HSIC_norm\n",
    "            \n",
    "        \n",
    "    return Adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions to compute dHcor on SDG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dHSIC_XX(group, groups_data, iterable):\n",
    "    \"\"\"Compute dHSIC(X,X) for variable X\"\"\"\n",
    "    group_arr = groups_data[group]\n",
    "    K = len(iterable)\n",
    "    indexes = np.arange(K)\n",
    "    \n",
    "    #compute individual HSIC_XX\n",
    "    HSIC_XX_list = []\n",
    "    for i in indexes:\n",
    "        k_list_XX = list((group_arr[i], group_arr[i]))\n",
    "        HSIC_XX = dHSIC(k_list_XX)\n",
    "        HSIC_XX_list.append(HSIC_XX)\n",
    "        \n",
    "    return HSIC_XX_list\n",
    "\n",
    "\n",
    "def compute_dHSIC_XXX(group, groups_data, iterable):\n",
    "    \"\"\"Compute dHSIC(X,X,X) for variable X\"\"\"\n",
    "    group_arr = groups_data[group]\n",
    "    K = len(iterable)\n",
    "    indexes = np.arange(K)\n",
    "    \n",
    "    #compute individual HSIC_XXX\n",
    "    HSIC_XXX_list = []\n",
    "    for i in indexes:\n",
    "        k_list_XXX = list((group_arr[i], group_arr[i], group_arr[i]))\n",
    "        HSIC_XXX = dHSIC(k_list_XXX)\n",
    "        HSIC_XXX_list.append(HSIC_XXX)\n",
    "        \n",
    "    return HSIC_XXX_list\n",
    "\n",
    "\n",
    "def compute_dHSIC_XXXX(group, groups_data, iterable):\n",
    "    \"\"\"Compute dHSIC(X,X,X,X) for variable X\"\"\"\n",
    "    group_arr = groups_data[group]\n",
    "    K = len(iterable)\n",
    "    indexes = np.arange(K)\n",
    "    \n",
    "    #compute individual HSIC_XXXX\n",
    "    HSIC_XXXX_list = []\n",
    "    for i in indexes:\n",
    "        k_list_XXXX = list((group_arr[i], group_arr[i], group_arr[i], group_arr[i]))\n",
    "        HSIC_XXXX = dHSIC(k_list_XXXX)\n",
    "        HSIC_XXXX_list.append(HSIC_XXXX)\n",
    "        \n",
    "    return HSIC_XXXX_list\n",
    "\n",
    "def compute_dHSIC_XXXXX(group, groups_data, iterable):\n",
    "    \"\"\"Compute dHSIC(X,X,X,X,X) for variable X\"\"\"\n",
    "    group_arr = groups_data[group]\n",
    "    K = len(iterable)\n",
    "    indexes = np.arange(K)\n",
    "    \n",
    "    #compute individual HSIC_XXXX\n",
    "    HSIC_XXXXX_list = []\n",
    "    for i in indexes:\n",
    "        k_list_XXXXX = list((group_arr[i], group_arr[i], group_arr[i], group_arr[i], group_arr[i]))\n",
    "        HSIC_XXXXX = dHSIC(k_list_XXXXX)\n",
    "        HSIC_XXXXX_list.append(HSIC_XXXXX)\n",
    "        \n",
    "    return HSIC_XXXXX_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dHSIC_links_MC_norm(group, groups_data, iterable, stop_after_2=False, n_perms=5000, alpha=0.05):\n",
    "    #For given dictionary groups_data, take nd.array corresponding to group\n",
    "    # ie. continents_prep_g_K['Europe']\n",
    "    group_arr = groups_data[group]\n",
    "    \n",
    "    K = len(iterable)   #number of total variables (17 goals, 76 targets)\n",
    "    edges = {}          #initialize dictionary with edges according to dependencies found\n",
    "    Adj2 = np.zeros((K,K))  #initialize KxK adjacency matrix for d=2\n",
    "    d = 2   #initial number of variables for dHSIC\n",
    "    e = 0  \n",
    "    \n",
    "    indexes = np.arange(K)    #create vector corresponding to indexes of iterable\n",
    "    #find all possible d-combinations of indexes without order\n",
    "    g_combinations = list(combinations_tuple(indexes, d))   \n",
    "    \n",
    "    weights_3 = {}\n",
    "    weights_4 = {}\n",
    "    weights_5 = {}\n",
    "    #iterate until no possible combinations of independent variables are left\n",
    "    while len(g_combinations) >0 :\n",
    "        print(\"combinations: \", d)\n",
    "        print(\"number of combinations available: \", len(g_combinations))\n",
    "        \n",
    "        f = 0\n",
    "        hsic_found = {}   #initialize dictionary with decision rule for each d-combination considered\n",
    "        #iterate over all combinations considered\n",
    "        for comb in g_combinations: \n",
    "            #create k_list[i] = Kernel from observed data for variable comb[i]\n",
    "            k_list = []\n",
    "            for i in range(d):\n",
    "                k_list.append(group_arr[comb[i]])\n",
    "            \n",
    "            #test joint independence: if reject H0, reject=1 (dependency found)\n",
    "            dHSIC_val, reject = joint_independence_test_MC(k_list, n_perms, alpha)   \n",
    "            \n",
    "            hsic_found[comb] = reject\n",
    "            if reject == 1:\n",
    "                e += 1\n",
    "                f += 1\n",
    "                edges[e] = tuple(iterable[i] for i in comb)  #add edge to graph according to dependency found\n",
    "                \n",
    "                ##### COMPUTE NORMALISATION\n",
    "                ### For each size choose corresponding dictionary with the pre-computed values of dHSIC(X,...,X)\n",
    "                ### for variable X.\n",
    "                if d == 2:\n",
    "                    HSIC_norm = dHSIC_val/np.sqrt(HSIC_XX[group][comb[0]]*HSIC_XX[group][comb[1]])\n",
    "                    Adj2[comb[0], comb[1]] = HSIC_norm\n",
    "                    Adj2[comb[1], comb[0]] = HSIC_norm\n",
    "                    \n",
    "                if d==3:\n",
    "                    HSIC_norm = dHSIC_val/(HSIC_XXX[group][comb[0]]*HSIC_XXX[group][comb[1]]*HSIC_XXX[group][comb[2]])**(1/d)\n",
    "                    weights_3[comb] = HSIC_norm  #add to dictionary of weights\n",
    "                    \n",
    "                if d==4:\n",
    "                    HSIC_norm = dHSIC_val/(HSIC_XXXX[group][comb[0]]*HSIC_XXXX[group][comb[1]]*HSIC_XXXX[group][comb[2]]*HSIC_XXXX[group][comb[3]])**(1/d)\n",
    "                    weights_4[comb] = HSIC_norm\n",
    "                    \n",
    "                if d==5:\n",
    "                    HSIC_norm = dHSIC_val/(HSIC_XXXX[group][comb[0]]*HSIC_XXXX[group][comb[1]]*HSIC_XXXX[group][comb[2]]*HSIC_XXXX[group][comb[3]]*HSIC_XXXX[group][comb[4]])**(1/d)\n",
    "                    weights_5[comb] = HSIC_norm\n",
    "                    \n",
    "                    \n",
    "        print(\"Edges found with \" ,d, \"nodes: \", f)\n",
    "        \n",
    "        if stop_after_2==True:\n",
    "            break\n",
    "            \n",
    "            \n",
    "        d +=1 #update d\n",
    "        if d==K+1:\n",
    "            break #stop iteration if d is greater than available variables\n",
    "        \n",
    "        #Find possible d-combinations of iterable. Note that if a dependency has already been found\n",
    "        #among elements of a combination <d, then we should not consider the combinations involving \n",
    "        #these elements\n",
    "        g_combinations_all = list(combinations_tuple(indexes, d))\n",
    "        g_combinations = copy.deepcopy(g_combinations_all)\n",
    "\n",
    "        for comb_n in g_combinations_all:\n",
    "            #consider all possible sub-combinations of d-1 elements in each comb of g_combinations_all\n",
    "            gg = list(combinations_tuple(comb_n, d-1)) \n",
    "            for l in range(len(gg)):\n",
    "                # for each sub_combination a dependency among its elements has already been found if \n",
    "                # that combination is not in hsic_found (so was already not considered in the previous \n",
    "                # step), or if it is but has value = 1 (there was a dependency only for the joint dist \n",
    "                # of all d-1 elements)\n",
    "                if (gg[l] in hsic_found and hsic_found[gg[l]]==1) or (gg[l] not in hsic_found):\n",
    "                    g_combinations.remove(comb_n)  #do not consider such combination\n",
    "                    break\n",
    "                   \n",
    "    return edges, Adj2, weights_3, weights_4, weights_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT\n",
    "continents_prep_g = pickle.load(open('/Users/saravallejomengod/MathsYear4/M4R/utils/Data/continents_prep_g.pkl', 'rb'))\n",
    "continents_prep_g_K = pickle.load(open('/Users/saravallejomengod/MathsYear4/M4R/utils/Data/continents_prep_g_K.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSIC_XX = {}\n",
    "HSIC_XX['Europe'] = compute_dHSIC_XX('Europe', continents_prep_g_K, goals)\n",
    "HSIC_XX['Asia'] = compute_dHSIC_XX('Asia', continents_prep_g_K, goals)\n",
    "HSIC_XX['Americas'] = compute_dHSIC_XX('Americas', continents_prep_g_K, goals)\n",
    "HSIC_XX['Africa'] = compute_dHSIC_XX('Africa', continents_prep_g_K, goals)\n",
    "\n",
    "HSIC_XXX = {}\n",
    "HSIC_XXX['Europe'] = compute_dHSIC_XXX('Europe', continents_prep_g_K, goals)\n",
    "HSIC_XXX['Asia'] = compute_dHSIC_XXX('Asia', continents_prep_g_K, goals)\n",
    "HSIC_XXX['Americas'] = compute_dHSIC_XXX('Americas', continents_prep_g_K, goals)\n",
    "HSIC_XXX['Africa'] = compute_dHSIC_XXX('Africa', continents_prep_g_K, goals)\n",
    "\n",
    "HSIC_XXXX = {}\n",
    "HSIC_XXXX['Europe'] = compute_dHSIC_XXXX('Europe', continents_prep_g_K, goals)\n",
    "HSIC_XXXX['Asia'] = compute_dHSIC_XXXX('Asia', continents_prep_g_K, goals)\n",
    "HSIC_XXXX['Americas'] = compute_dHSIC_XXXX('Americas', continents_prep_g_K, goals)\n",
    "HSIC_XXXX['Africa'] = compute_dHSIC_XXXX('Africa', continents_prep_g_K, goals)\n",
    "\n",
    "HSIC_XXXXX = {}\n",
    "HSIC_XXXXX['Europe'] = compute_dHSIC_XXXXX('Europe', continents_prep_g_K, goals)\n",
    "HSIC_XXXXX['Asia'] = compute_dHSIC_XXXXX('Asia', continents_prep_g_K, goals)\n",
    "HSIC_XXXXX['Americas'] = compute_dHSIC_XXXXX('Americas', continents_prep_g_K, goals)\n",
    "HSIC_XXXXX['Africa'] = compute_dHSIC_XXXXX('Africa', continents_prep_g_K, goals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combinations:  2\n",
      "number of combinations available:  136\n",
      "Edges found with  2 nodes:  79\n",
      "combinations:  3\n",
      "number of combinations available:  93\n",
      "Edges found with  3 nodes:  9\n",
      "combinations:  4\n",
      "number of combinations available:  64\n",
      "Edges found with  4 nodes:  2\n",
      "combinations:  5\n",
      "number of combinations available:  22\n",
      "Edges found with  5 nodes:  1\n",
      "combinations:  6\n",
      "number of combinations available:  3\n",
      "Edges found with  6 nodes:  0\n"
     ]
    }
   ],
   "source": [
    "edges_e_n, Adj2_e_n, weights_3_e, weights_4_e, weights_5_e = dHSIC_links_MC_norm('Europe', continents_prep_g_K, goals, stop_after_2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combinations:  2\n",
      "number of combinations available:  136\n",
      "Edges found with  2 nodes:  50\n",
      "combinations:  3\n",
      "number of combinations available:  181\n",
      "Edges found with  3 nodes:  17\n",
      "combinations:  4\n",
      "number of combinations available:  121\n",
      "Edges found with  4 nodes:  1\n",
      "combinations:  5\n",
      "number of combinations available:  29\n",
      "Edges found with  5 nodes:  0\n",
      "combinations:  6\n",
      "number of combinations available:  1\n",
      "Edges found with  6 nodes:  0\n"
     ]
    }
   ],
   "source": [
    "edges_af_n, Adj2_af_n, weights_3_af, weights_4_af, weights_5_af = dHSIC_links_MC_norm('Africa', continents_prep_g_K, goals, stop_after_2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combinations:  2\n",
      "number of combinations available:  136\n",
      "Edges found with  2 nodes:  42\n",
      "combinations:  3\n",
      "number of combinations available:  241\n",
      "Edges found with  3 nodes:  25\n",
      "combinations:  4\n",
      "number of combinations available:  230\n",
      "Edges found with  4 nodes:  13\n",
      "combinations:  5\n",
      "number of combinations available:  90\n",
      "Edges found with  5 nodes:  10\n",
      "combinations:  6\n",
      "number of combinations available:  6\n",
      "Edges found with  6 nodes:  0\n"
     ]
    }
   ],
   "source": [
    "edges_am_n, Adj2_am_n, weights_3_am, weights_4_am, weights_5_am = dHSIC_links_MC_norm('Americas', continents_prep_g_K, goals, stop_after_2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combinations:  2\n",
      "number of combinations available:  136\n",
      "Edges found with  2 nodes:  81\n",
      "combinations:  3\n",
      "number of combinations available:  73\n",
      "Edges found with  3 nodes:  8\n",
      "combinations:  4\n",
      "number of combinations available:  30\n",
      "Edges found with  4 nodes:  2\n",
      "combinations:  5\n",
      "number of combinations available:  2\n",
      "Edges found with  5 nodes:  0\n"
     ]
    }
   ],
   "source": [
    "edges_as_n, Adj2_as_n, weights_3_as, weights_4_as, weights_5_as = dHSIC_links_MC_norm('Asia', continents_prep_g_K, goals, stop_after_2=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that nodes are numbered starting from 0, so 0 corresponds to SDG 1 and 16 to SDG 17. Also note that these results slightly vary to the results found in notebooks starting with 1 and presented in the thesis, since there is a randomness factor when doing the permutation tests. However, this variation is very low (1 pairwise edge is added/missing comparing both results. But it has a bit greater impact on higher-order interactions). Normally one would compute this directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 5, 16): 0.517318054591152,\n",
       " (0, 6, 13): 0.46565221826107767,\n",
       " (0, 8, 9): 0.4855786511680134,\n",
       " (1, 2, 5): 0.5117630841043357,\n",
       " (1, 4, 14): 0.32336411664661036,\n",
       " (1, 5, 6): 0.48317467340643266,\n",
       " (1, 6, 12): 0.5270250677830337,\n",
       " (1, 10, 14): 0.3032539995214665,\n",
       " (1, 14, 15): 0.36503028070463184,\n",
       " (2, 5, 10): 0.4295696150362931,\n",
       " (2, 10, 15): 0.5519834911339592,\n",
       " (4, 10, 14): 0.27449756803056524,\n",
       " (5, 10, 15): 0.4347460656101391,\n",
       " (5, 10, 16): 0.44219811564085276,\n",
       " (6, 7, 13): 0.4495866060970891,\n",
       " (10, 14, 15): 0.30585180679362606,\n",
       " (10, 14, 16): 0.3043820799018939}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_3_af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 4, 11): 0.552351596173919,\n",
       " (0, 11, 15): 0.5627071991783804,\n",
       " (1, 12, 13): 0.3906794535682024,\n",
       " (3, 11, 15): 0.6357569402738706,\n",
       " (4, 6, 11): 0.4516841797203104,\n",
       " (4, 6, 12): 0.39051880033222053,\n",
       " (5, 12, 13): 0.3523590851688421,\n",
       " (6, 11, 12): 0.3720122046173314,\n",
       " (9, 11, 12): 0.3578678840545791}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_3_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1, 6): 0.6047761884803207,\n",
       " (0, 1, 8): 0.6302108447198438,\n",
       " (0, 1, 9): 0.5731888868902573,\n",
       " (0, 1, 10): 0.5332564985330487,\n",
       " (0, 1, 16): 0.6724391744105209,\n",
       " (0, 4, 6): 0.5548690674375382,\n",
       " (0, 4, 9): 0.5468807220610385,\n",
       " (0, 4, 10): 0.5137497079388597,\n",
       " (0, 4, 16): 0.6450670837497104,\n",
       " (0, 8, 10): 0.5186391105539766,\n",
       " (1, 3, 6): 0.6833118984489388,\n",
       " (1, 3, 10): 0.627795301112587,\n",
       " (1, 3, 11): 0.66581928085346,\n",
       " (1, 4, 6): 0.6170896148196292,\n",
       " (1, 4, 11): 0.6054462250366052,\n",
       " (1, 4, 16): 0.7152479855906755,\n",
       " (1, 5, 8): 0.6294410556139898,\n",
       " (1, 7, 10): 0.571160941400739,\n",
       " (2, 3, 10): 0.6510798255471015,\n",
       " (2, 7, 10): 0.5904656998985888,\n",
       " (3, 8, 10): 0.6276459517509272,\n",
       " (4, 5, 8): 0.6082053198863145,\n",
       " (4, 6, 12): 0.5831154456316728,\n",
       " (4, 12, 16): 0.6818523610355206,\n",
       " (6, 13, 14): 0.32475416271481883}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_3_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 4, 5): 0.45650625678626594,\n",
       " (1, 3, 9): 0.5245308694715953,\n",
       " (1, 3, 14): 0.40536011209370926,\n",
       " (1, 4, 5): 0.49521807996044764,\n",
       " (1, 5, 9): 0.398821708829959,\n",
       " (1, 5, 12): 0.4754667283906946,\n",
       " (2, 4, 8): 0.621002164856549,\n",
       " (2, 5, 12): 0.4860589135268572}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_3_as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT\n",
    "groups_prep_g = pickle.load(open('/Users/saravallejomengod/MathsYear4/M4R/utils/Data/groups_prep_g.pkl', 'rb'))\n",
    "groups_prep_g_K = pickle.load(open('/Users/saravallejomengod/MathsYear4/M4R/utils/Data/groups_prep_g_K.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSIC_XX['Low Income'] = compute_dHSIC_XX('Low Income', groups_prep_g_K, goals)\n",
    "HSIC_XX['Lower middle Income'] = compute_dHSIC_XX('Lower middle Income', groups_prep_g_K, goals)\n",
    "HSIC_XX['Upper middle Income'] = compute_dHSIC_XX('Upper middle Income', groups_prep_g_K, goals)\n",
    "HSIC_XX['High Income'] = compute_dHSIC_XX('High Income', groups_prep_g_K, goals)\n",
    "\n",
    "HSIC_XXX['Low Income'] = compute_dHSIC_XXX('Low Income', groups_prep_g_K, goals)\n",
    "HSIC_XXX['Lower middle Income'] = compute_dHSIC_XXX('Lower middle Income', groups_prep_g_K, goals)\n",
    "HSIC_XXX['Upper middle Income'] = compute_dHSIC_XXX('Upper middle Income', groups_prep_g_K, goals)\n",
    "HSIC_XXX['High Income'] = compute_dHSIC_XXX('High Income', groups_prep_g_K, goals)\n",
    "\n",
    "HSIC_XXXX['Low Income'] = compute_dHSIC_XXXX('Low Income', groups_prep_g_K, goals)\n",
    "HSIC_XXXX['Lower middle Income'] = compute_dHSIC_XXXX('Lower middle Income', groups_prep_g_K, goals)\n",
    "HSIC_XXXX['Upper middle Income'] = compute_dHSIC_XXXX('Upper middle Income', groups_prep_g_K, goals)\n",
    "HSIC_XXXX['High Income'] = compute_dHSIC_XXXX('High Income', groups_prep_g_K, goals)\n",
    "\n",
    "HSIC_XXXXX['Low Income'] = compute_dHSIC_XXXXX('Low Income', groups_prep_g_K, goals)\n",
    "HSIC_XXXXX['Lower middle Income'] = compute_dHSIC_XXXXX('Lower middle Income', groups_prep_g_K, goals)\n",
    "HSIC_XXXXX['Upper middle Income'] = compute_dHSIC_XXXXX('Upper middle Income', groups_prep_g_K, goals)\n",
    "HSIC_XXXXX['High Income'] = compute_dHSIC_XXXXX('High Income', groups_prep_g_K, goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combinations:  2\n",
      "number of combinations available:  136\n",
      "Edges found with  2 nodes:  29\n",
      "combinations:  3\n",
      "number of combinations available:  339\n",
      "Edges found with  3 nodes:  14\n",
      "combinations:  4\n",
      "number of combinations available:  522\n",
      "Edges found with  4 nodes:  5\n",
      "combinations:  5\n",
      "number of combinations available:  441\n",
      "Edges found with  5 nodes:  0\n",
      "combinations:  6\n",
      "number of combinations available:  197\n",
      "Edges found with  6 nodes:  0\n",
      "combinations:  7\n",
      "number of combinations available:  40\n",
      "Edges found with  7 nodes:  0\n",
      "combinations:  8\n",
      "number of combinations available:  2\n",
      "Edges found with  8 nodes:  0\n"
     ]
    }
   ],
   "source": [
    "edges_li_n, Adj2_li_n, weights_3_li, weights_4_li, weights_5_li = dHSIC_links_MC_norm('Low Income', \n",
    "                                                                                      groups_prep_g_K, \n",
    "                                                                                      goals, stop_after_2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combinations:  2\n",
      "number of combinations available:  136\n",
      "Edges found with  2 nodes:  46\n",
      "combinations:  3\n",
      "number of combinations available:  204\n",
      "Edges found with  3 nodes:  11\n",
      "combinations:  4\n",
      "number of combinations available:  178\n",
      "Edges found with  4 nodes:  2\n",
      "combinations:  5\n",
      "number of combinations available:  66\n",
      "Edges found with  5 nodes:  0\n",
      "combinations:  6\n",
      "number of combinations available:  6\n",
      "Edges found with  6 nodes:  0\n"
     ]
    }
   ],
   "source": [
    "edges_lmi_n, Adj2_lmi_n, weights_3_lmi, weights_4_lmi, weights_5_lmi = dHSIC_links_MC_norm('Lower middle Income', \n",
    "                                                                                           groups_prep_g_K, \n",
    "                                                                                           goals, stop_after_2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combinations:  2\n",
      "number of combinations available:  136\n",
      "Edges found with  2 nodes:  53\n",
      "combinations:  3\n",
      "number of combinations available:  171\n",
      "Edges found with  3 nodes:  17\n",
      "combinations:  4\n",
      "number of combinations available:  118\n",
      "Edges found with  4 nodes:  3\n",
      "combinations:  5\n",
      "number of combinations available:  29\n",
      "Edges found with  5 nodes:  0\n"
     ]
    }
   ],
   "source": [
    "edges_umi_n, Adj2_umi_n, weights_3_umi, weights_4_umi, weights_5_umi = dHSIC_links_MC_norm('Upper middle Income', \n",
    "                                                                                           groups_prep_g_K, \n",
    "                                                                                           goals, stop_after_2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combinations:  2\n",
      "number of combinations available:  136\n",
      "Edges found with  2 nodes:  104\n",
      "combinations:  3\n",
      "number of combinations available:  20\n",
      "Edges found with  3 nodes:  3\n",
      "combinations:  4\n",
      "number of combinations available:  2\n",
      "Edges found with  4 nodes:  0\n"
     ]
    }
   ],
   "source": [
    "edges_hi_n, Adj2_hi_n, weights_3_hi, weights_4_hi, weights_5_hi = dHSIC_links_MC_norm('High Income', \n",
    "                                                                                      groups_prep_g_K, \n",
    "                                                                                      goals, stop_after_2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 4, 12): 0.6239370808692326,\n",
       " (1, 3, 6): 0.7567830838257555,\n",
       " (1, 6, 12): 0.6482964120031154,\n",
       " (1, 11, 12): 0.6667488682682163,\n",
       " (1, 11, 13): 0.6400832264922859,\n",
       " (2, 3, 5): 0.6681130629693043,\n",
       " (2, 3, 16): 0.7972400104463777,\n",
       " (2, 5, 10): 0.5914487852105873,\n",
       " (2, 10, 16): 0.7063161038246938,\n",
       " (3, 4, 12): 0.6492884307223529,\n",
       " (3, 5, 6): 0.6299411787658997,\n",
       " (3, 6, 12): 0.6678646963866877,\n",
       " (3, 10, 16): 0.7298234325828132,\n",
       " (3, 11, 12): 0.6750313470657403}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_3_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 2, 13): 0.4620546229131828,\n",
       " (0, 5, 13): 0.37792117039986256,\n",
       " (0, 6, 9): 0.44296474160873256,\n",
       " (0, 8, 16): 0.6317223908936701,\n",
       " (1, 4, 9): 0.5191576951616631,\n",
       " (5, 6, 13): 0.35358162107848556,\n",
       " (5, 8, 13): 0.38288156977222665,\n",
       " (5, 13, 15): 0.41217160851517043,\n",
       " (6, 9, 15): 0.4919653145114121,\n",
       " (7, 11, 12): 0.5474540113553859,\n",
       " (8, 10, 12): 0.4645026361825696}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_3_lmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 2, 4): 0.5479393928424914,\n",
       " (0, 4, 6): 0.49091632128235474,\n",
       " (0, 4, 14): 0.2858799637348693,\n",
       " (1, 8, 13): 0.47989477588395063,\n",
       " (2, 3, 4): 0.6447820130197153,\n",
       " (2, 7, 10): 0.525601143862533,\n",
       " (3, 4, 6): 0.5780166684406807,\n",
       " (3, 4, 9): 0.49481048587931775,\n",
       " (3, 4, 14): 0.34251047036383586,\n",
       " (3, 6, 11): 0.5592412120771778,\n",
       " (3, 9, 12): 0.4985805530522867,\n",
       " (3, 11, 12): 0.5621065836689697,\n",
       " (4, 8, 9): 0.45448539290394274,\n",
       " (4, 8, 14): 0.3137443561891768,\n",
       " (4, 9, 12): 0.4240525024497302,\n",
       " (4, 9, 15): 0.4397844011643624,\n",
       " (4, 10, 14): 0.2615948782997213}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_3_umi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 12, 15): 0.32601931374922755,\n",
       " (2, 12, 14): 0.2660676425867357,\n",
       " (5, 12, 13): 0.24556083667245573}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_3_hi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute missing ones that we had found before. (This is only for my thesis, normally one would compute them all at the same time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6841105320530013"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_LI_1_4_13 = [groups_prep_g_K['Low Income'][0], groups_prep_g_K['Low Income'][3], groups_prep_g_K['Low Income'][12]]\n",
    "LI_1_4_13 = dHSIC(K_LI_1_4_13)\n",
    "LI_1_4_13/(HSIC_XXX['Low Income'][0]*HSIC_XXX['Low Income'][3]*HSIC_XXX['Low Income'][12])**(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.680039853358151"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_LI_2_4_6 = [groups_prep_g_K['Low Income'][1], groups_prep_g_K['Low Income'][3], groups_prep_g_K['Low Income'][5]]\n",
    "LI_2_4_6 = dHSIC(K_LI_2_4_6)\n",
    "LI_2_4_6/(HSIC_XXX['Low Income'][1]*HSIC_XXX['Low Income'][3]*HSIC_XXX['Low Income'][5])**(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6228308988880011"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_LI_2_6_7 = [groups_prep_g_K['Low Income'][1], groups_prep_g_K['Low Income'][5], groups_prep_g_K['Low Income'][6]]\n",
    "LI_2_6_7 = dHSIC(K_LI_2_6_7)\n",
    "LI_2_6_7/(HSIC_XXX['Low Income'][1]*HSIC_XXX['Low Income'][5]*HSIC_XXX['Low Income'][6])**(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6021597855498495"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_LI_5_12_13 = [groups_prep_g_K['Low Income'][4], groups_prep_g_K['Low Income'][11], groups_prep_g_K['Low Income'][12]]\n",
    "LI_5_12_13 = dHSIC(K_LI_5_12_13)\n",
    "LI_5_12_13/(HSIC_XXX['Low Income'][4]*HSIC_XXX['Low Income'][11]*HSIC_XXX['Low Income'][12])**(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4423548755100861"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_LMI_1_9_14 = [groups_prep_g_K['Lower middle Income'][0], groups_prep_g_K['Lower middle Income'][8], \n",
    "               groups_prep_g_K['Lower middle Income'][13]]\n",
    "LMI_1_9_14 = dHSIC(K_LMI_1_9_14)\n",
    "LMI_1_9_14/(HSIC_XXX['Lower middle Income'][0]*HSIC_XXX['Lower middle Income'][8]*HSIC_XXX['Lower middle Income'][13])**(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6002648988969547"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_LMI_2_8_12 = [groups_prep_g_K['Lower middle Income'][1], groups_prep_g_K['Lower middle Income'][7], \n",
    "               groups_prep_g_K['Lower middle Income'][11]]\n",
    "LMI_2_8_12 = dHSIC(K_LMI_2_8_12)\n",
    "LMI_2_8_12/(HSIC_XXX['Lower middle Income'][1]*HSIC_XXX['Lower middle Income'][7]*HSIC_XXX['Lower middle Income'][11])**(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4640786282402432"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_LMI_6_8_12 = [groups_prep_g_K['Lower middle Income'][5], groups_prep_g_K['Lower middle Income'][7], \n",
    "               groups_prep_g_K['Lower middle Income'][11]]\n",
    "LMI_6_8_12 = dHSIC(K_LMI_6_8_12)\n",
    "LMI_6_8_12/(HSIC_XXX['Lower middle Income'][5]*HSIC_XXX['Lower middle Income'][7]*HSIC_XXX['Lower middle Income'][11])**(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35956952115257407"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_UMI_6_12_14 = [groups_prep_g_K['Lower middle Income'][5], groups_prep_g_K['Lower middle Income'][11], \n",
    "               groups_prep_g_K['Lower middle Income'][13]]\n",
    "UMI_6_12_14 = dHSIC(K_UMI_6_12_14)\n",
    "UMI_6_12_14/(HSIC_XXX['Lower middle Income'][5]*HSIC_XXX['Lower middle Income'][11]*HSIC_XXX['Lower middle Income'][13])**(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3050612923460161"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_LMI_4_11_15 = [groups_prep_g_K['Upper middle Income'][3], groups_prep_g_K['Upper middle Income'][10], \n",
    "               groups_prep_g_K['Upper middle Income'][14]]\n",
    "LMI_4_11_15 = dHSIC(K_LMI_4_11_15)\n",
    "LMI_4_11_15/(HSIC_XXX['Upper middle Income'][3]*HSIC_XXX['Upper middle Income'][10]*HSIC_XXX['Upper middle Income'][14])**(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47958315233127197"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
